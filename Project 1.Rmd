---
title: "Project 1 - Wine Quality"
author: "Moustapha Dieng"
date: "February 18, 2018"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100) # Adjust output width
```

# Part I.  Regression

Abstract: <br/>
Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009]

Description: <br/>
In this project, I will be combining the two datasets and studying the effects of some attributes on the wine quality. The wines are ranked on a scale from 0 to 10.

Source: <br/>
  P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. <br/>
  Modeling wine preferences by data mining from physicochemical properties. <br/>
  In Decision Support Systems, Elsevier, 47(4):547-553. ISSN: 0167-9236. <br/>

  Available at: <br/>
  [@Elsevier] http://dx.doi.org/10.1016/j.dss.2009.05.016 <br/>
  Download datasets [here](http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/).
  
## Initial Setup

```{r}
# Load required librariess
library(ggplot2) # Needed for ggplot 
library(gridExtra) # Needed for grid.arrange
library(corrplot) # Needed for corrplot
library(caret)
library(class)
```

I will be combining both datasets for the regression part of this project.

```{r}
white <- read.csv("data/winequality-white.csv", header=TRUE, sep=";") # Read in white wine csv file.
red <- read.csv("data/winequality-red.csv", header=TRUE, sep=";") # Read in red wine csv file
wine1 <- rbind(white, red) # Combines the two data frames by rows. 
```

## Examining the Dataset
 
**Structure of the dataset**

```{r}
str(wine1) # Structure of dataset
```

From the structure, we can see the dimension of the dataset, names and types of the attributes as well as a preview of their values.

**Columns description**

- Fixed acidity (g(tartaric acid)/dm3): The predominant fixed acids found in wines are tartaric, malic, citric, and succinic. 

- Volatile acidity (g(acetic acid)/dm3): A measure of the wine's volatile (or gaseous) acids. The primary volatile acid in wine is acetic acid, which is also the primary acid associated with the smell and taste of vinegar.

- Citric acid  (g/dm3): Citric acid is a weak organic acid, which is often used as a natural preservative or additive to food or drink to add a sour taste to food.

- Residual sugar (g/dm3): How much sugar is left in the wine after fermentation is complete. The amount of residual sugar tells you how sweet the wine is going to be.

- Chlorides (g(sodium chloride)/dm3): The amount of chlorides ions in the wine.

- Free sulfur dioxide (mg/dm3): Free sulfites are those available to react and thus exhibit both germicidal and antioxidant properties.

- Total sulfur dioxide (mg/dm3): Free and bound sulfites. The bound sulfites are those that have reacted (both reversibly and irreversibly) with other molecules within the wine medium.

- Density (g/cm3)

- pH

- Sulphates (g(potassium sulphate)/dm3): Preservatives that are widely used in winemaking (and most food industries) for its antioxidant and antibacterial properties.

- Alcohol (vol.%)

- Quality (score between 0 and 10)

**Target column**

The target column will be quality.

**Summary of dataset**

```{r}
summary(wine1) # Structure of dataset
```

The summary provides useful statistics. For example, we can see that the minimum and maximum scores given to wines are 3 and 9 respectively. From the description of the dataset, we are provided with the information that wines are scored on a 0 to 10 scale.

**Preview of dataset**

```{r}
head(wine1) # Preview of dataset
```

**Edge cases**

```{r}
# Display number of wines with poor rating
paste("Number of poor quality wines: ", sum(wine1$quality == 3))
# Display number of wines with excellent rating
paste("Number of excellent quality wines: ", sum(wine1$quality == 9))
```

From these results, we can see that very few wines are rated as poor (score of 3) and even fewer as excellent (score of 9).

## Graphs

**Histogram**

```{r}
ggplot(wine1, aes(x = quality)) +
  geom_histogram(binwidth = 0.5) +
  geom_bar(color = "blue", fill = "white") +
  scale_x_continuous(breaks = round(seq(min(wine1$quality), max(wine1$quality), by = 1),1)) +
  labs(title = "Wine") # Bar plot for wine scores
```

With the graph, we can visualize the summary of the quality scores of the wines. We can easily see that the mean is between 5 and 6 for the whole dataset.

**Correlation plot**

The correlation plot will be saved to then loaded from a png file for better readibility.

```{r}
png(height = 600, width = 800, file = "corrPlot.png")
corr <- cor(wine1) # Compute matrix of correlation
corrplot(corr, method = "color", addCoef.col = "grey") # Save plot to png file for better readibility
dev.off() # Shutdown currrent graphics device
```

![](corrPlot.png)

From the correlation plot, we can observe a few strong correlations: <br/>
- Residual.sugar and density @ 0.55 <br/>
- free.sulfur.dioxide and total.sulfur.dioxide @ 0.72 <br/>
- alcohol and density @ -0.69 <br/>
It's also interesting to note that quality has a somewhat strong positive correlation with only one other variable, alcohol @ 0.44.

## Algorithms

### Linear Regression: Model 1 -  quality~alcohol

```{r}
set.seed(1234) # Set seed to 1234 to ensure reproducibility of results
i <- sample(nrow(wine1), nrow(wine1)*.75, replace=FALSE) # Sample from dataframe
train <-wine1[i,] # Initiliaze train set
test <- wine1[-i,] # Initialize test set
lm1 <- lm(quality~alcohol, data=train) # Create linear model lm1 from train set
summary(lm1) # Summary of linear model
```

The p-value being much less than significance level of 0.05 indicates that there exists a strong relationship between quality and alcohol. However, looking at the R-squared which is only .2016, we know that only 20.16% of the total variation in quality can be explained by the linear relationship between quality and alcohol. We will therefore come up with a new linear model after calculating the MSE of our current model.

**Mean Squared Error of lm1:**

```{r}
mean(lm1$residuals^2) # Display MSE of lm1
```

### Linear Regression: Model 2 - quality~.

```{r}
lm2 <- lm(quality~., data = train) # Create linear model lm2 from train set
summary(lm2) # Summary of linear model
```

By analyzing our summary, we can see that all variables but citric.acid and chlorides have strong significance. Moreover, our R-squared is now at 0.2975 vs .2016 for model 1 which shows that this model explain the variation in quality better.

**Mean Squared Error of lm2:**

```{r}
mean(lm2$residuals^2) # Display MSE of lm2
```

The MSE decreased from 0.608906 for model 1 to .537894 which demonstrates that the second model is better than the first.

### Linear Regression: Model 3 - quality~.-citric.acid-chlorides

```{r}
lm3 <- lm(quality~.-citric.acid-chlorides, data=train) # Create linear model lm3 from train set
summary(lm3) # Summary of linear model
```

Although all the variables in this model are significant, its R-squared is slightly lower than the second linear model .2972 vs .2975. So model 2 is still better in that aspect.

**Mean Squared Error of lm3:**

```{r}
mean(lm3$residuals^2) # Display MSE of lm3
```

Model 3 MSE is slightly higher than model 2's .5360198 vs 0.5357894. In conclusion, model 2 is the clear winner as far as linear regression models are concerned. We can easily verify this with an analysis of variance table.

**Anova**

```{r}
anova(lm1, lm2, lm3)
```

As expected, the anova results show model 2 having the lowest RSS.

**Correlation of lm2**

```{r}
pred <- predict(lm2, newdata=test) # Predict results based on test data
paste("Correlation for linear model 2: ", cor(pred, test$quality) * 100, "%") # Display correlation
```


### Knn Regression

**Choosing k: 10-fold cross-validation**

```{r}
scaled_df <- data.frame(scale(wine1[, 1:12])) # Scale the data first
scaled_train1 <- scaled_df[i,] # Set train set
scaled_test1 <- scaled_df[-i,] # Set test set
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3) # Controls the computational nuances of the train() method, 10-fold, repeat 3 times
knn_fit <- train(quality~., data = scaled_train1, method = "knn",
 trControl=trctrl,
 preProcess = c("center", "scale"),
 tuneLength = 10) # Train the classifier
knn_fit # Checking results
```

**Knn regression with k = 9**

```{r}
knnreg1 <- knnreg(scaled_train1[, 1:11],scaled_train1[, 12], k = 9) # 1:11 for training set predictors and 12 (quality) for response
```

**Knn is not model-based therefore no summary is displayed**

According to the cross-validation, our optimal k = 9.

**Correlation of Knn Regression**

```{r}
pred1 <- predict(knnreg1, scaled_test1[, 1:11]) # Predict results
paste("Correlation for knn, k = 9: ", cor(pred1, test$quality) * 100, "%") # Displays correlation results
```

**Final comments**

Knn with k = 9 has better correlation results than lm2 (formula(quality~.)). Knn with k = 9 correlation is around 56.94% while lm2 has correlation is around 52.36%. It doesn't seem possible to reliably predict the wine quality given the predictors we used.

# Part II. Classification

I will be using the same dataset for the classification part but I will be adding a categorical variable to differenciate the wines by color.

**Add color attribute**

```{r}
white$color <- 'white' # Add categorical attribute 'color' to white wine dataset
white$color <- as.factor(white$color) # Transfrom the attribute color into a factor
red$color <- 'red' # Add categorical attribute 'color' to red wine dataset
red$color <- as.factor(red$color) # Transfrom the attribute color into a factor
wine2 <- rbind(white, red) # Combines the two data frames by rows.
```

## Examining the Dataset

**Structure of the dataset**

```{r}
str(wine2) # Structure of dataset
```

**Columns description**

The columns are the same as in part I. However, I have added the 'color' column to categorize the wines by color.

**Target column**

The target column will be color. I will be attempting to predict the wine color using all other variables as predictors.

**Summary of dataset**

```{r}
summary(wine2) # Summary of dataset
```

Note: The only difference is the added column 'color'. The red wines account for about 1/3 of the data.

**Preview of dataset**

```{r}
wine2[sample(nrow(wine2), 10), ] # Random sample of dataset
```

**Best/worst wines**

```{r}
# Find the percentage of highest rated wine that are white
paste("White wines make up ", 
      sum(wine2[which(wine2$quality == 9), 13] == 'white') /
        nrow(wine2[which(wine2$quality == 9), ]) * 100,
      "% of the excellent wines.")
# Find the percentage of worst rated wine that are white
paste("White wines make up ", 
      sum(wine2[which(wine2$quality == 3), 13] == 'white') /
        nrow(wine2[which(wine2$quality == 3), ]) * 100,
      "% of the worst wines.")
```

## Graphs

**Histogram**

```{r}
plot_white <- ggplot(white, aes(x = quality)) +
 geom_histogram(binwidth = 0.5) +
 geom_bar(color = "blue", fill = "white") +
 scale_x_continuous(breaks = round(seq(min(wine2$quality), max(wine2$quality), by = 1), 1)) +
 labs(title = "White Wine") # Histogram for white wine scores
plot_red <- ggplot(red, aes(x = quality)) +
 geom_histogram(binwidth = 0.5) +
 geom_bar(color = "blue", fill = "red") +
 scale_x_continuous(breaks = round(seq(min(wine2$quality), max(wine2$quality), by = 1), 1)) +
 labs(title = "Red Wine")  # Histogram for red wine scores
grid.arrange(plot_white, plot_red, ncol = 2)  # Arrange plot side by side
```

From the graph, we can visualize the distribution of scores by wine color. There are a few distinctions to note which can help in identifying the type of wine.

**Plot of Alcohol~Density **

```{r}
ggplot(wine2, aes(x = density, y = alcohol, color = color)) +
  geom_point(alpha = 0.1, position = position_jitter(h = 0), size = 2) +
  geom_smooth(method = 'glm') +
  coord_cartesian(xlim=c(min(wine2$density), 1.005), ylim=c(min(wine2$alcohol), max(wine2$alcohol))) +
  xlab('Density') +
  ylab('Alcohol') +
  ggtitle('Alcohol~Density')
```

The graph shows that red wines tend to be denser than white wines with the same alcohol volume.

## Algorithms

### Logistical Regression

```{r}
set.seed(1234) # Set seed
i <- sample(nrow(wine2), nrow(wine2)*.67, replace=FALSE) # Sample from df
train <- wine2[i,] # Initialize train set
test <- wine2[-i,] # Initialize test set
glm1 <- glm(color~., data = train, family = binomial) # Create logistical regression
summary(glm1) # Summary of model
```

The model created is much better than the one created from the intercept alone as the residual deviance is much lower than the null deviance. Although, not all attributes are significant, I will be using this model to predict the type of wine.

**Accuracy of prediction**

```{r}
probs1 <- predict(glm1, newdata=test, type='response') # Computes the probilities based on test data
pred2 <- ifelse(probs1 > 0.5, "red", "white" ) # Classify probabilities
table(Predicted = pred2, Actual = test$color) # Prediction table
paste("Accuracy of prediction: ", mean(pred2==test$color) * 100, "%") # Displays accuracy of prediction
```

### Knn Classification

**Choosing k: 10-fold cross-validation**

```{r}
set.seed(4752)  # Set seed for reproducibility
i <- sample(nrow(wine2), nrow(wine2)*.67, replace=FALSE) # Sample from data frame
train <- wine2[i, ]
test <- wine2[-i, ]
trctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 3) # Controls the computational nuances of the train() method, 10-fold, repeat 3 times
knn_fit <- train(color~., data = train, method = "knn",
                 trControl=trctrl,
                 preProcess = c("center", "scale"),
                 tuneLength = 10) # Train the classifier
knn_fit # Checking results
```

**Knn classification with k = 11**

```{r}
ind <- sample(2, nrow(scaled_df), replace = TRUE, prob=c(0.67, 0.33)) # Get random sample
scaled_train2 <- scaled_df[ind == 1, 1:12] # Initiliaze train set
scaled_test2 <- scaled_df[ind == 2, 1:12] # Initiliaze test set
trainLabels <- wine2[ind == 1, 13] # Set training label
testLabels <- wine2[ind == 2, 13] # Set test label
pred2 <- knn(train = scaled_train2, test = scaled_test2, cl = trainLabels, k = 11) # Predict results
```

**Knn is not model-based therefore no summary is displayed**

**Accuracy of prediction**

```{r}
table(Predicted = pred2, Actual = testLabels) # Prediction table
paste("Accuracy of prediction: ", mean(pred2 == testLabels) * 100, "%") # Displays accuracy of prediction
```

**Final comments**

Both the logistical regression and knn algorithms are able to predict the type of wine extremely accurately: 99.40% for logistical regression vs 99.27% for knn with k = 11. In conclusion, logistical regression performs slightly better in this scenario.




